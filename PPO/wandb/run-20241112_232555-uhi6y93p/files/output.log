Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6447/6447 [00:02<00:00, 2165.01 examples/s]

Filter (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6447/6447 [00:00<00:00, 11359.62 examples/s]
  0%|                                                                                                                                                                    | 0/2500 [00:00<?, ?it/s]

  0%|                                                                                                                                                                    | 0/2500 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
  0%|                                                                                                                                                          | 1/2500 [00:11<8:10:15, 11.77s/it]Traceback (most recent call last):
  File "/mnt/batch/tasks/shared/LS_root/mounts/clusters/yutong-8gpus-training/code/Users/yutong.jiang2/Pretrain_LLM/PPO/PPO.py", line 55, in <module>
    trainer.train()
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 577, in train
    self.generate_completions(sampling=True)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 619, in generate_completions
    for batch in self.eval_dataloader:
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/accelerate/data_loader.py", line 552, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 672, in _next_data
    index = self._next_index()  # may raise StopIteration
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 620, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/sampler.py", line 278, in __iter__
    sampler_iter = iter(self.sampler)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/sampler.py", line 112, in __iter__
    return iter(range(len(self.data_source)))
TypeError: object of type 'NoneType' has no len()
{'eps': 1, 'objective/kl': 9.298662007495295e-06, 'objective/entropy': 31.461416244506836, 'objective/non_score_reward': -4.649330094252946e-07, 'objective/rlhf_reward': -0.5734387040138245, 'objective/scores': -0.5734382271766663, 'policy/approxkl_avg': 0.0007598996162414551, 'policy/clipfrac_avg': 0.0015723269898444414, 'loss/policy_avg': -0.00045333802700042725, 'loss/value_avg': 2.1398510932922363, 'val/clipfrac_avg': 0.0, 'policy/entropy_avg': 0.5352588891983032, 'val/ratio': 0.9992657899856567, 'val/ratio_var': 5.722236892324872e-06, 'val/num_eos_tokens': 0, 'lr': 0.0, 'episode': 12, 'epoch': 0.0}