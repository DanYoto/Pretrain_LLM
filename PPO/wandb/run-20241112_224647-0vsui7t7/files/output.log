Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                          | 0/29181 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/mnt/batch/tasks/shared/LS_root/mounts/clusters/yutong-8gpus-training/code/Users/yutong.jiang2/Pretrain_LLM/PPO/PPO.py", line 46, in <module>
    trainer.train()
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 339, in train
    data = next(iter_dataloader)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 286, in repeat_generator
    yield from dataloader
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/accelerate/data_loader.py", line 552, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/data/data_collator.py", line 271, in __call__
    batch = pad_without_fast_tokenizer_warning(
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3499, in pad
    padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(
  File "/home/azureuser/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2923, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
===training policy===